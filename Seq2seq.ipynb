{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzr5sz186WxRhAz7P/I7P2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kolayn808/textattack_test/blob/main/Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка необходимых библиотек"
      ],
      "metadata": {
        "id": "w70nu5NcRwIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovhLQF_07ruZ",
        "outputId": "bebd8cf8-b9bf-4534-cd7a-ec443512a3a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка моделей и стоп-слов.  "
      ],
      "metadata": {
        "id": "OpkyzSKRR-iX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2IqBZQNuFra",
        "outputId": "bd592167-da91-4630-e406-ffefe99184bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import nltk\n",
        "import gensim.downloader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        "# Загрузка модели Word2Vec\n",
        "word2vec_model = gensim.downloader.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Загрузка токенизатора и модели seq2seq\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  # Замените на желаемую модель seq2seq\n",
        "my_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")  # Замените на желаемую модель seq2seq\n",
        "\n",
        "# Загрузка стоп-слов\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция атаки"
      ],
      "metadata": {
        "id": "ZSuZ5fjcSgtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def attack_sequence_to_sequence_model(input_text, model, max_attempts=10, max_edits=5):\n",
        "    original_text = input_text\n",
        "    for _ in range(max_attempts):\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True)\n",
        "        num_edits = random.randint(1, max_edits)\n",
        "        for _ in range(num_edits):\n",
        "            input_word = random.choice(input_ids[0].tolist())\n",
        "            input_word = tokenizer.convert_ids_to_tokens([input_word])[0]\n",
        "            if input_word not in stop_words and input_word in word2vec_model:\n",
        "                similar_words = word2vec_model.most_similar(input_word, topn=5)\n",
        "                new_word = random.choice(similar_words)[0]\n",
        "                input_text = input_text.replace(input_word, new_word, 1)\n",
        "\n",
        "        # Генерируем новый текст с моделью seq2seq\n",
        "        generated_ids = my_model.generate(input_ids, max_length=128, num_return_sequences=1)\n",
        "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        if generated_text != original_text:\n",
        "            return generated_text  # Возвращаем атакованный текст\n",
        "    return None  # Возвращаем None, если атака не удалась"
      ],
      "metadata": {
        "id": "6LSXbYYSSV2e"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример использования"
      ],
      "metadata": {
        "id": "e5GyhKjCSoJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "original_text = \"Two men wearing swim trunks jump in the air at a moderately populated beach\"\n",
        "attacked_text = attack_sequence_to_sequence_model(original_text, my_model)\n",
        "if attacked_text:\n",
        "    print(\"Оригинальный текст:\", original_text)\n",
        "    print(\"Атакованный текст:\", attacked_text)\n",
        "else:\n",
        "    print(\"Атака не удалась.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSpvTPAQSXjm",
        "outputId": "0a533938-4eda-4157-f370-70e9e2b10748"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оригинальный текст: Two men wearing swim trunks jump in the air at a moderately populated beach\n",
            "Атакованный текст: Zwei Männer in swim trunk jump in the air at a moderately populated beach.\n"
          ]
        }
      ]
    }
  ]
}